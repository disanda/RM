# Training with the Hessian Penalty: Tips and Tricks

Below are some useful tips for using our portable Hessian Penalty implementations in your codebases.

* **Important Note on Randomization in Forward Passes:** Computing the Hessian Penalty involves evaluating expressions of the form `G(z+x) - 2G(z) + G(z-x)`. Consider if you have dropout layers in your network `G`. Then the dropout mask sampled during each of these three forward passes will be different; the masks need to be the same for the Hessian approximation to be correct. In general, to make the computation accurate, it is important that any forward pass randomness in `G` is made deterministic when calling `hessian_penalty`. There are several ways to do this. Returning to the dropout example, the `G` that is passed into `hessian_penalty` should either be in `eval` mode (so there is no randomness) or the dropout mask should be explicitly seeded. For example, you can modify `G`'s forward pass to take a `dropout_seed` argument that directly seeds the dropout mask. Then, you would call `hessian_penalty(G, z, dropout_seed=seed)`, where `seed` is randomized before every call to `hessian_penalty`. Other common examples of the forward pass randomness that you would need to make deterministic: adding/concatenating noise to activations (as in the TensorFlow implementation of StyleGAN) and the reparameterization trick (as in VAEs). Similarly, if any parameters in your network `G` automatically change from performing a forward pass (e.g., moving averages), then you would need to ensure those parameters do not update when `hessian_penalty` is called. **Basically, you can be confident `hessian_penalty(G, z, **G_kwargs)` will be correct if `G(z, **G_kwargs)` always produces *exactly* the same computation when called multiple times with `z` fixed and `**G_kwargs` fixed.**

* **My neural net's forward pass takes multiple input arguments; how do I pass them to `hessian_penalty`?** Just pass them by name. See an example at the top of our [PyTorch](hessian_penalty_pytorch.py) and [TensorFlow](hessian_penalty_tf.py) implementations.

* **I don't want to take the Hessian Penalty with respect to the first input argument in my network's forward pass. How do I take it with respect to an arbitrary network input?** Let's say you have a network `G(a, b, c)` and you want to take the Hessian Penalty with respect to `b`. Right before you call `hessian_penalty`, you can define `G_b = lambda b: G(a=a, b=b, c=c)` and call `hessian_penalty(G=G_b, z=b)`. The Hessian Penalty is always taken with respect to the first input to `G` (which is passed via the `z` argument).

* **How do I apply the Hessian Penalty to multiple intermediate activations simultaneously/efficiently?** Both our [TensorFlow](hessian_penalty_tf.py) and [PyTorch](hessian_penalty_pytorch.py) implementations support this feature. To use it, modify your network (call it `G`) so its forward pass can return a *list* of intermediate activations you wish to regularize. For example, you could add a `return_activations` argument to `G`'s forward pass that, when `True`, returns all of the activations you want to regularize. Then you can call `hessian_penalty(G, z, return_activations=True)` and you are good to go!

* If you're using our PyTorch implementation of `hessian_penalty` with multiple GPUs, **make sure your calls to `hessian_penalty` are wrapped in `nn.DataParallel`**. Otherwise, your GPU memory will be heavily imbalanced across GPUs and slow down your code substantially.

* Some code written in older versions of TensorFlow uses a `reuse` parameter when defining their model's forward pass (passed to `tf.variable_scope`) to ensure that the same weights are used whenever a forward pass is called. If your code uses this pattern, you should pass `reuse=True` to `hessian_penalty` to make sure you don't accidentally instantiate multiple networks.

* **If you are fine-tuning a pre-trained model with the Hessian Penalty, it is critical that you ramp-up the loss weighting from zero.** You can do this by setting the loss weighting (let's call it `hp_lambda`) for training iteration `t` as `hp_lambda_t = max_hp_lambda * min(1, t/T)`, where you can try settings like `T=1e5` or `1e6` (feel free to try increasing the loss weighting in a non-linear fashion, too), and `max_hp_lambda` is the final loss weighting after ramping-up. You shouldn't need to do this if you're training a model from scratch with the Hessian Penalty.

* **Does fine-tuning with the Hessian Penalty work better than training from scratch?** It's not clear. In some cases, such as CLEVR-Simple and CLEVR-Complex, fine-tuning with the Hessian Penalty seems to give qualitatively better disentanglement. But, in the case of CLEVR-1FOV, we got better results training from scratch.

* **The Hessian Penalty is decreasing, but disentanglement isn't improving. What's going on?** We sometimes observe that this can happen, and it seems to be the result of a degenerate optimization of the Hessian Penalty. Try regularizing deeper layers in the network, and make sure you aren't regularizing right after a linear layer, such as a fully-connected or convolutional layer. Try regularizing immediately after normalization layers instead.

* **Be careful when changing the `epsilon` hyperparameter.** There is an `O(1/epsilon**4)` reliance on that hyperparameter, so decreasing it without decreasing `hp_lambda` can cause explosions. Similarly, increasing `epsilon` without increasing `hp_lamdba` can cause the Hessian Penalty to do nothing since the effective learning rate would greatly decrease.

* **I'm getting GPU out-of-memory errors. Help!** See our fifth and sixth bullet point above. The Hessian Penalty does use more GPU memory since it needs multiple forward passes to estimate the second directional derivatives. You can reduce the memory footprint by regularizing earlier layers in the network, reducing the number of directions sampled (controlled by the `k` argument, but `k` must be at least 2) or by only applying the Hessian Penalty to a subset of batch elements (I have not personally tried this last recommendation, but I expect it should work).
